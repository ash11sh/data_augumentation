{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a4uB7StBxAIp"
   },
   "source": [
    "# Data Augumentation\n",
    "Training deep-learning models with more diverse data-set implies towards its accuracy. It prevents model from being over-fit. Augmentation multiplies the data-set by applying few transformation techniques like random crop, flip, Normalise, Resizing, Padding.\n",
    "\n",
    "<p align=\"center\"><img src=\"https://live.staticflickr.com/65535/49135954821_9a0c878231_d.jpg\" width=\"600\"></p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y5v5JXfexAIr",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Collecting torch==1.3.1+cpu\n",
      "\u001b[?25l  Downloading https://download.pytorch.org/whl/cpu/torch-1.3.1%2Bcpu-cp36-cp36m-linux_x86_64.whl (111.8MB)\n",
      "\u001b[K    40% |█████████████                   | 45.2MB 887kB/s eta 0:01:16   8% |██▊                             | 9.3MB 1.5MB/s eta 0:01:08    24% |████████                        | 27.9MB 1.3MB/s eta 0:01:06    25% |████████                        | 28.1MB 1.6MB/s eta 0:00:52    29% |█████████▍                      | 32.8MB 1.6MB/s eta 0:00:49    37% |████████████                    | 41.9MB 1.4MB/s eta 0:00:50"
     ]
    }
   ],
   "source": [
    "!pip install torch==1.3.1+cpu torchvision==0.4.2+cpu -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7n3bq183xAIz",
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install opencv-python==4.1.1.26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "68cwQwsSxAI3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import PIL\n",
    "import cv2\n",
    "import torchvision.transforms.functional as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YdO3pgyCxAI7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def imshow(img, title):\n",
    "    plt.title(title)\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 250
    },
    "colab_type": "code",
    "id": "n6V7oFTExAI_",
    "outputId": "b7535dc1-79ee-420e-a2c5-27b331a98fbc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "img = PIL.Image.open('img_3.png')\n",
    "imshow(img, 'normal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-CUYBjOYxAJD",
    "tags": []
   },
   "source": [
    "## brightness factor\n",
    "brightness_factor (float) – How much to adjust the brightness. Can be any non negative number. 0 gives a black image, 1 gives the original image while 2 increases the brightness by a factor of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V4QGsDxRxAJE",
    "outputId": "b72a20b6-650e-4381-a529-dfa9cf6b52be",
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_1 = tf.adjust_brightness(img, brightness_factor = 2.5)\n",
    "\n",
    "im = tf.adjust_brightness(img, brightness_factor = 5.76)\n",
    "\n",
    "imshow(img_1,'brightness')\n",
    "imshow(im , 'heavy bright')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iufiriC9xAJI",
    "tags": []
   },
   "source": [
    "## Resize\n",
    "If size is a sequence like\n",
    "            (h, w), the output size will be matched to this. If size is an int,\n",
    "            the smaller edge of the image will be matched to this number maintaing\n",
    "            the aspect ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r1lsrAkExAJJ",
    "outputId": "64d458eb-d7b6-4933-f70a-d7aae1dfcd3c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_2 = tf.resize(img,(900,800))\n",
    "im_r = tf.resize(img, 45)\n",
    "imshow(img_2,'resize')\n",
    "imshow(im_r, 'resized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z8lVaA9XxAJN",
    "tags": []
   },
   "source": [
    "## resized_crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tKd6StIbxAJO",
    "outputId": "5d4dae6e-0ed3-4807-ffdc-b6ec2bba9b61",
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_3 = tf.resized_crop(img,100,600,120,120,1000)\n",
    "imshow(img_3,'resized_crop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aJRITy_zxAJU",
    "tags": []
   },
   "source": [
    "## Rotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2ZhrSVKsxAJX",
    "outputId": "d9bec687-eec7-43e6-d1b4-f266a874fbad",
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_4 = tf.rotate(img, angle=70)\n",
    "im_h = tf.rotate(img, angle=290)\n",
    "imshow(img_4, 'rotate=70')\n",
    "imshow(im_h, 'rotate=290')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iftMbWFGxAJb",
    "tags": []
   },
   "source": [
    "## horizontal flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2jw6E3QuxAJc",
    "outputId": "556f6255-a74e-4eaa-a05f-a14746307d6c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_5 = tf.hflip(img)\n",
    "imshow(img,'normal')\n",
    "imshow(img_5, 'h_flip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qKg5C-PixAJh",
    "tags": []
   },
   "source": [
    "## padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WI_ferMXxAJi",
    "outputId": "3437f276-2d91-403e-9583-df26a9ea5f44",
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_6 = tf.pad(img,(300,300,400,400),(255,107,10), padding_mode='edge')\n",
    "img_10 = tf.pad(img, (300,300,400,400),padding_mode='reflect')\n",
    "imshow(img_6, 'padded_edge')\n",
    "imshow(img_10, 'padded_reflect')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IlV-biKyxAJl",
    "tags": []
   },
   "source": [
    "## vflip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ikh5rl6rxAJn",
    "outputId": "6e0523f8-1772-4f2c-b162-a3732d886003",
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_7 = tf.vflip(img)\n",
    "imshow(img,'normal')\n",
    "imshow(img_7,'v_flip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 253
    },
    "colab_type": "code",
    "id": "KThfI9YdxAJr",
    "outputId": "9000e33a-c975-4cf3-f604-5634df77629d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "image = PIL.Image.open('img_1.jpg')\n",
    "imshow(image, 'lalaland')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9iJCm_0ExAJu",
    "tags": []
   },
   "source": [
    "## gray_scale sconversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sAFdwBKmxAJw",
    "outputId": "067796d4-876f-4a65-eb80-30774adcf023",
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''if num_output_channels = 1 : returned image is single channel\n",
    "\n",
    "if num_output_channels = 3 : returned image is 3 channel with r = g = b'''\n",
    "\n",
    "img_8 = tf.to_grayscale(img,3)\n",
    "img_9 = tf.to_grayscale(image,1) \n",
    "imshow(img_9, 'gray1')\n",
    "imshow(img_8, 'gray2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Oc7w377ZxAJ0",
    "tags": []
   },
   "source": [
    "## contrast\n",
    "contrast_factor (float) – How much to adjust the contrast. Can be any non negative number. 0 gives a solid gray image, 1 gives the original image while 2 increases the contrast by a factor of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "riATQxh2xAJ1",
    "outputId": "77c5e9bd-4c35-4d38-a597-274b2b507851",
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_11 = tf.adjust_contrast(image, contrast_factor=1)\n",
    "img_12 = tf.adjust_contrast(image, contrast_factor=2)\n",
    "imshow(img_11,'contrst')\n",
    "imshow(img_12, 'contsrt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EXkPMBMBxAJ4",
    "tags": []
   },
   "source": [
    "## gama correction\n",
    "gamma (float) – Non negative real number, same as \\gammaγ in the equation. gamma larger than 1 make the shadows darker, while gamma smaller than 1 make dark regions lighter.\n",
    "\n",
    "gain (float) – The constant multiplier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WGD4dTTLxAJ6",
    "outputId": "c47834c0-e4cc-4374-b3e7-7519cdee4457",
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_13 = tf.adjust_gamma(img, gamma = 0.3, gain=1)\n",
    "img_14 = tf.adjust_gamma(img, gamma = 3, gain=1)\n",
    "imshow(img_13,'less_shadow')\n",
    "imshow(img_14,'more_shadow')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LFP99trlxAJ-",
    "tags": []
   },
   "source": [
    "## hue\n",
    "hue_factor (float) – How much to shift the hue channel. Should be in [-0.5, 0.5]. 0.5 and -0.5 give complete reversal of hue channel in HSV space in positive and negative direction respectively. 0 means no shift. Therefore, both -0.5 and 0.5 will give an image with complementary colors while 0 gives the original image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hCBrZcmYxAJ_",
    "outputId": "ca6f6413-6a07-47b5-bc79-de076eb002d9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_15 = tf.adjust_hue(img,hue_factor=0.2)\n",
    "img_16 = tf.adjust_hue(img, hue_factor=-0.1)\n",
    "imshow(img_15, 'hue')\n",
    "imshow(img_16, 'huee')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WbqLPmr5xAKH",
    "tags": []
   },
   "source": [
    "## saturation\n",
    "saturation_factor (float) – How much to adjust the saturation. 0 will give a black and white image, 1 will give the original image while 2 will enhance the saturation by a factor of 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bT5FXlzSxAKU",
    "outputId": "14e4b740-3be2-4014-b0c7-f91bfed26c45",
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_17 = tf.adjust_saturation(img,saturation_factor=0)\n",
    "img_18 = tf.adjust_saturation(img, saturation_factor=2)\n",
    "imshow(img_17,'saturation')\n",
    "imshow(img_18, 'more saturation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7VpNAeu6xAKm",
    "tags": []
   },
   "source": [
    "## affine transformation\n",
    "  resample (``PIL.Image.NEAREST`` or ``PIL.Image.BILINEAR`` or ``PIL.Image.BICUBIC``, optional):\n",
    "            An optional resampling filter.\n",
    "            See `filters`_ for more information.\n",
    "            If omitted, or if the image has mode \"1\" or \"P\", it is set to ``PIL.Image.NEAREST``.\n",
    "        fillcolor (int): Optional fill color for the area outside the transform in the output image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Or8Igw13xAKo",
    "outputId": "a5a08fa2-2a3f-47e4-d7b3-5bc6d24bca66",
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_19 = tf.affine(img,angle=40, translate=(6,3), scale=0.8,\n",
    "                shear=-20, resample=PIL.Image.BICUBIC, fillcolor=(256,142,154))\n",
    "\n",
    "img_20 = tf.affine(img,angle=140, translate=(6,3), scale=0.8,\n",
    "                shear=-20, resample=PIL.Image.NEAREST, fillcolor=(0,142,154))\n",
    "imshow(img_19, 'affine')\n",
    "imshow(img_20, 'affine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OLsYYFfTx59_"
   },
   "source": [
    "## normalise\n",
    "Normalize a tensor image with mean and standard deviation.\n",
    "    Given mean: ``(M1,...,Mn)`` and std: ``(S1,..,Sn)`` for ``n`` channels, this transform\n",
    "    will normalize each channel of the input ``torch.*Tensor`` i.e.\n",
    "    ``input[channel] = (input[channel] - mean[channel]) / std[channel]``\n",
    "\n",
    "note::\n",
    "        This transform acts out of place, i.e., it does not mutates the input tensor.\n",
    "\n",
    "    Args:\n",
    "        tensor : input tensor image\n",
    "        mean (sequence): Sequence of means for each channel.\n",
    "        std (sequence): Sequence of standard deviations for each channel.\n",
    "        inplace(bool,optional): Bool to make this operation in-place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2Cv1Yi1KxAKu",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#convert PIL image to tensor\n",
    "tensor = tf.to_tensor(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rG5aprCEyNzB"
   },
   "outputs": [],
   "source": [
    "#normalize the tensor image\n",
    "normalize = tf.normalize(tensor,(0.485, 0.456, 0.406), (0.229, 0.224, 0.225))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gotwh6SDy6td"
   },
   "outputs": [],
   "source": [
    "#conversion back to PIL image from tensors and normalized\n",
    "t2PIL = tf.to_pil_image(tensor)\n",
    "N2PIL = tf.to_pil_image(normalize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483
    },
    "colab_type": "code",
    "id": "zKqs3xoR6NBq",
    "outputId": "5ea82e33-c71c-4c5c-9209-ee543dd45e91"
   },
   "outputs": [],
   "source": [
    "imshow(t2PIL,'tensor to PIL')\n",
    "imshow(N2PIL, 'normalized to PIL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U6WBlIob6OYG"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "notebook.ipynb",
   "provenance": []
  },
  "deepnote_published_at": 1574922344829,
  "deepnote_published_id": "d497fa873621450ea2c51ea83a49d069",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
